# -*- coding: utf-8 -*-
"""Webinar_VIT_TensorFlow2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10v1KWIjX2Ad8tFDQLXf_1Y4hrTg6krT5

#Introduction to Tensors

Tensor is a container to store data and is  a generalized form of matrices.

**Tensor Attributes:**
1. Dimension/rank/axis (ndim)
2. Shape (shape)
3. Data type (dtype)
"""

import numpy as np

"""**A scalar tensor or 0D tensor or scalar**"""

x = np.array(48)
print("shape - {}, dimensions - {}, data type - {}".format(x.shape, x.ndim, x.dtype))

"""**Vector or 1D tensor**
1. only one axis

**Note:** It is a 1D Tensor and the vector itself is a 4D.
"""

x = np.array([1,2,3,4])
print("shape - {}, dimensions - {}, data type - {}".format(x.shape, x.ndim, x.dtype))

"""**Matrix or 2D Tensor**
1. Array of vectors 
2. Two axis: rows and columns
"""

x = np.array([[1,2,3],
             [4,5,6],
             [5,6,7]])
print("shape - {}, dimensions - {}, data type - {}".format(x.shape, x.ndim, x.dtype))

"""**3D Tensors**
1. Array of Matrices
2. In the example shown below it is an array of four 3x3 matices
"""

x = np.array([[[1,2,3],
              [4,5,6],
              [7,8,9]],
             [[1,2,3],
              [4,5,6],
              [7,8,9]],
             [[1,2,3],
              [4,5,6],
              [7,8,9]],
             [[1,2,3],
              [4,5,6],
              [7,8,9]]])
print("shape - {}, dimensions - {}, data type - {}".format(x.shape, x.ndim, x.dtype))

"""##Common Tensor Operations:
1. tf.constant and tf.variable 
2. tf.zeros and tf.ones
3. tf.concat
4. tf.reshape
5. tf.cast
"""

#importing tensorflow
import tensorflow as tf
# creating variable and constant tensors
a_var = tf.Variable([[1,2],
                    [3,4]])
b_const = tf.constant([[1,2],
                      [3,4]])

# creating tensors of 0's and 1's 
ones = tf.ones([2,2], dtype = tf.float32)
zeros = tf.zeros([2,2], dtype = tf.int32)
print("Matrix of one",ones)
print("======================================================")
print("Matrix of zeros",zeros)
print("======================================================")

# concatenating two tensors
rows = tf.concat(values=[a_var, b_const], axis=0)
print("Concatenating rows",rows)
print("======================================================")
columns = tf.concat(values=[a_var, b_const], axis=1)
print("Concatenating columns",columns)
print("======================================================")

# reshaping tensors
reshaped = tf.reshape(rows, [-1])
print("Flattening the tensor", reshaped)
print("======================================================")

# casting the tensors
flt_to_int = tf.cast(ones, tf.int32)
print("Casting from float to int", flt_to_int)

"""##Linear algebric operations
1. tf.matmul and elementwise multiplication
2. tf.transpose
3. tf.linalg.det
"""

# Matrix multiplication and elementwise multiplication
ab_mul = tf.matmul(a_var, b_const)
print("Matrix multiplications",ab_mul)
print("======================================================")
ab_ele = a_var*b_const
print("Elementwise product",ab_ele)
print("======================================================")

# Finding transpose of a matrix
transpose = tf.transpose(a_var)
print("Transpose of the matrix",transpose)
print("======================================================")

# Determinant of matrix 
#NOTE: Input tensor should be a float type
a_var_det = tf.cast(a_var, tf.float32)
det = tf.linalg.det(a_var_det)
print("Determinant of matrix",det)
print("======================================================")

"""#Gradients using tensorflow

A sigmoid activation function outputs values from 0 to 1.

g(x) = 1/(1 + e^(-x)) = y

g'(x) = g(x)(1 - g(x))

Example: 
Let, x = 0.5

g(0.5) = 1/(1 + e^(-0.5)) = 0.62245

g'(x) = g(x)(1 - g(x)) = 0.2350
"""

def get_gradient(x):
  with tf.GradientTape() as tape:
    y = tf.nn.sigmoid(x)
  
  gradient = tape.gradient(y, x)
  return gradient, y

test = tf.Variable([0.5, 0.6, 0.7])
gradient, y = get_gradient(test)

print("Values of sigmoid function output", y)
print("======================================================")
print("After differentiating sigmoid function", gradient)

"""#Data loading and preprocessing

For this tutorial are using mnist haddenwritten digits dataset. The dataset consists of 60,000 training and 10,000 testing images of size 28x28.
"""

import matplotlib.pyplot as plt


# Loading dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
print("shape of training set features",x_train.shape)
print("shape of training set labels", y_train.shape)
print("shape of testing set features", x_test.shape)
print("shape of testing set labels", y_test.shape)

# Normalizing

x_train, x_test = x_train/255.0, x_test/255.0

ex_images, ex_labels = x_train[0:10], y_train[0:10]
plt.figure(figsize = (10,10))
for n in range(10):
  ax = plt.subplot(2, 5, n+1)
  plt.imshow(ex_images[n], cmap = 'binary')
  plt.title(ex_labels[n])
  plt.axis('off')

# Adding a new axis for channel
x_train, x_test = x_train[..., tf.newaxis], x_test[..., tf.newaxis]
print("======================================================")
print("shape of training set features",x_train.shape)
print("shape of training set labels", y_train.shape)
print("shape of testing set features", x_test.shape)
print("shape of testing set labels", y_test.shape)

# Batching and shuffling using tf.data API
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(32)
test_ds =  tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(10000).batch(32)
print(train_ds.element_spec)

"""#Three Ways of creating a model in TensorFlow:
1. sequential
2. Functional
3. Subclassing
"""

# Loss function
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

#Hyper-parameters
eph = 5
batch = 32
lr = 0.001

#Optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)

"""##Sequential API

Sequential API is the most basic way of bulding and training a neural network. This method stacks the layers one upon another automatically.

**Advantages:**
1. Simple and easy to use

**Disadvantages**
1. Construction of complicated architectures are not possible
2. Lesser control over the network
"""

#Creating the model
model_seq = tf.keras.Sequential([
                             tf.keras.layers.Conv2D(32, 3, input_shape = x_train.shape[1:] , activation = 'relu'),
                             tf.keras.layers.Flatten(),
                             tf.keras.layers.Dense(128, activation = 'relu'),
                             tf.keras.layers.Dense(10)
                            ])
model_seq.summary()
tf.keras.utils.plot_model(model_seq)

#Model fit and compile
model_seq.compile(optimizer= optim,
              loss= loss_fn,
              metrics=['accuracy'])
model_seq.fit(x_train, y_train, batch_size = batch, epochs= eph)

"""##Functional API

This method provides the added advantage of creating complex architectures (ResNet, Inception module, etc,.).
"""

#Creating the model
inp = tf.keras.Input(shape = x_train.shape[1:])
conv2d_layer1 = tf.keras.layers.Conv2D(32, 3, input_shape = x_train.shape[1:] , activation = 'relu')(inp)
flatten = tf.keras.layers.Flatten()(conv2d_layer1)
dense_layer2 = tf.keras.layers.Dense(128, activation = 'relu')(flatten)
out = tf.keras.layers.Dense(10)(dense_layer2)
model_fun = tf.keras.Model(inputs = inp, outputs = out)

model_fun.summary()
tf.keras.utils.plot_model(model_seq)

#Model fit and compile
model_fun.compile(optimizer= optim,
              loss= loss_fn,
              metrics=['accuracy'])
model_fun.fit(x_train, y_train, batch_size = batch, epochs= eph)

"""##Subclassing API

* In this section we create a network using subclassing and train using custom loop instead of model.fit.
* This methods gives a greater control for building complicated end to end networks with custom back-propagation.

![alt text](https://drive.google.com/uc?export=view&id=1X_NqvE6Axv_8U76qbrsRK3vvN_6MfbX5)
"""

class SubModel(tf.keras.Model):
  def __init__(self):
    super(SubModel,self).__init__()
    #Defining the layers
    self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')
    self.flatten = tf.keras.layers.Flatten()
    self.dense1 = tf.keras.layers.Dense(128, activation='relu')
    self.dense2 = tf.keras.layers.Dense(10)
  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.dense1(x)
    return self.dense2(x)

model_subclass = SubModel()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

def back_prop(images, labels):
  with tf.GradientTape() as tape:
    predictions = model_subclass(images, training = True) #Step 1: Forward pass
    loss = loss_fn(labels, predictions) #Step 2: Loss calculation
  gradients = tape.gradient(loss, model_subclass.trainable_variables) #Step 3: Change in loss w.r.t parameters
  optim.apply_gradients(zip(gradients, model_subclass.trainable_variables)) #Step 4: Updating

  train_loss(loss)
  train_acc(labels, predictions)

def test_loop(images, labels):
  predictions = model_subclass(images, training = False)
  loss_test = loss_fn(labels, predictions)

  test_loss(loss_test)
  test_acc(labels, predictions)

EPOCHS = 5

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()
  train_acc.reset_states()
  test_loss.reset_states()
  test_acc.reset_states()

  for images, labels in train_ds:
    back_prop(images, labels)

  for test_images, test_labels in test_ds:
    test_loop(test_images, test_labels)

  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print(template.format(epoch + 1,
                        train_loss.result(),
                        train_acc.result() * 100,
                        test_loss.result(),
                        test_acc.result() * 100))